{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "from flock import FlockEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_td3():\n",
    "    n_agents = 2\n",
    "\n",
    "    env = FlockEnv(num_agents=n_agents, num_obstacles=0, width=300, height=300)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    action_noise = NormalActionNoise(mean=np.zeros((n_agents, 2)), sigma=0.1 * np.ones((n_agents, 2)))\n",
    "\n",
    "    model = TD3(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        verbose=1,\n",
    "        learning_rate=1e-3,\n",
    "        buffer_size=int(1e6),\n",
    "        action_noise=action_noise,\n",
    "        learning_starts=25000,\n",
    "        batch_size=128,\n",
    "        gamma=0.99,\n",
    "        tau=0.005,\n",
    "    )\n",
    "\n",
    "    eval_env = FlockEnv(num_agents=n_agents, num_obstacles=0, width=300, height=300)\n",
    "\n",
    "    eval_callback = EvalCallback(eval_env, log_path=\"../models/\", eval_freq=1000,\n",
    "        deterministic=True, render=False, n_eval_episodes=1)\n",
    "\n",
    "    model.learn(total_timesteps=125000, callback=eval_callback)\n",
    "    model.save(\"../models/flock_td3_2\")\n",
    "\n",
    "def evaluate():\n",
    "    env = FlockEnv(num_agents=2, num_obstacles=0, width=300, height=300)\n",
    "\n",
    "    model = TD3.load(\"../models/flock_td3_2\")\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "    print(f\"Mean reward: {mean_reward}, Std reward: {std_reward}\")\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kzq/flock/.venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kzq/flock/.venv/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    episodes        | 4    |\n",
      "|    fps             | 620  |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 4000 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    episodes        | 8    |\n",
      "|    fps             | 698  |\n",
      "|    time_elapsed    | 11   |\n",
      "|    total_timesteps | 7730 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 12    |\n",
      "|    fps             | 646   |\n",
      "|    time_elapsed    | 17    |\n",
      "|    total_timesteps | 11156 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 16    |\n",
      "|    fps             | 643   |\n",
      "|    time_elapsed    | 23    |\n",
      "|    total_timesteps | 15156 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 20    |\n",
      "|    fps             | 654   |\n",
      "|    time_elapsed    | 28    |\n",
      "|    total_timesteps | 18911 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    episodes        | 24    |\n",
      "|    fps             | 652   |\n",
      "|    time_elapsed    | 35    |\n",
      "|    total_timesteps | 22911 |\n",
      "------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-80.75 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -80.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.11    |\n",
      "|    critic_loss     | 0.771    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 999      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 430      |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 26911    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.87    |\n",
      "|    critic_loss     | 2.38     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1910     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-49.79 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -49.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.9    |\n",
      "|    critic_loss     | 13.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1999     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28000, episode_reward=145.22 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 145      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.9    |\n",
      "|    critic_loss     | 11.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2999     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=29000, episode_reward=-54.83 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -54.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -33.7    |\n",
      "|    critic_loss     | 18.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3999     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=86.46 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 86.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -51.4    |\n",
      "|    critic_loss     | 17.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4999     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 297      |\n",
      "|    time_elapsed    | 103      |\n",
      "|    total_timesteps | 30815    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -65.7    |\n",
      "|    critic_loss     | 79       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5814     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-85.37 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -85.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -73      |\n",
      "|    critic_loss     | 151      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5999     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=53.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 53.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -101     |\n",
      "|    critic_loss     | 106      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6999     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=55.42 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 55.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -109     |\n",
      "|    critic_loss     | 34.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7999     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-23.07 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -23.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -108     |\n",
      "|    critic_loss     | 27.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8999     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 230      |\n",
      "|    time_elapsed    | 150      |\n",
      "|    total_timesteps | 34815    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -137     |\n",
      "|    critic_loss     | 50.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9814     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-23.89 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -23.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -135     |\n",
      "|    critic_loss     | 92.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9999     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=6.91 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 6.91     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -143     |\n",
      "|    critic_loss     | 47.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 10999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=-67.84 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -67.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -178     |\n",
      "|    critic_loss     | 15.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-73.97 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -74      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -196     |\n",
      "|    critic_loss     | 26       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 201      |\n",
      "|    time_elapsed    | 192      |\n",
      "|    total_timesteps | 38815    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -203     |\n",
      "|    critic_loss     | 18.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 13814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-46.35 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -46.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -219     |\n",
      "|    critic_loss     | 25.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 13999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-55.06 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -55.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -197     |\n",
      "|    critic_loss     | 24.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 14999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=-54.53 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -54.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -217     |\n",
      "|    critic_loss     | 21.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 15999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-38.99 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -39      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -204     |\n",
      "|    critic_loss     | 15.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 16999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 180      |\n",
      "|    time_elapsed    | 236      |\n",
      "|    total_timesteps | 42815    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -234     |\n",
      "|    critic_loss     | 64.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 17814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-35.92 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -35.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -233     |\n",
      "|    critic_loss     | 34.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 17999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-64.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -64.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -234     |\n",
      "|    critic_loss     | 26.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 18999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-92.76 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -92.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -241     |\n",
      "|    critic_loss     | 20.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 19999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-89.71 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -89.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -240     |\n",
      "|    critic_loss     | 29       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 20999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 283      |\n",
      "|    total_timesteps | 46815    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -269     |\n",
      "|    critic_loss     | 20.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 21814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=-97.51 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -97.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -252     |\n",
      "|    critic_loss     | 27.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 21999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-52.24 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -253     |\n",
      "|    critic_loss     | 37.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 22999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=-110.44 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -110     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -260     |\n",
      "|    critic_loss     | 26.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 23999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-138.95 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -139     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -290     |\n",
      "|    critic_loss     | 92       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 24999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 326      |\n",
      "|    total_timesteps | 50815    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -295     |\n",
      "|    critic_loss     | 26.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 25814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-110.43 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -110     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -295     |\n",
      "|    critic_loss     | 30.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 25999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-104.43 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 27.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 26999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=-117.79 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -118     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 23.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 27999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-78.33 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -78.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 19.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 28999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 146      |\n",
      "|    time_elapsed    | 373      |\n",
      "|    total_timesteps | 54815    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 135      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 29814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-112.09 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -367     |\n",
      "|    critic_loss     | 55.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 29999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-122.90 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -123     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 38.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 30999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=-115.60 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -116     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 26.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 31999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=-107.17 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -107     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -307     |\n",
      "|    critic_loss     | 15.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 32999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 140      |\n",
      "|    time_elapsed    | 418      |\n",
      "|    total_timesteps | 58815    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 46.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 33814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=-125.20 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -125     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 38.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 33999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-117.33 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -117     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 116      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 34999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=-123.20 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -123     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 52.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 35999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=-166.49 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -166     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 32       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 36999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 136      |\n",
      "|    time_elapsed    | 461      |\n",
      "|    total_timesteps | 62815    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 35.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 37814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=-131.33 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -131     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 33.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 37999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-182.73 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -183     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 34.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 38999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-115.38 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -115     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -377     |\n",
      "|    critic_loss     | 29.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 39999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-123.82 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -124     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -370     |\n",
      "|    critic_loss     | 52.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 40999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 132      |\n",
      "|    time_elapsed    | 503      |\n",
      "|    total_timesteps | 66815    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 35.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 41814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=-117.38 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -117     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -367     |\n",
      "|    critic_loss     | 47.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 41999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-130.16 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -130     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 38.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 42999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=-108.16 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 38.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 43999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-90.47 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -90.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 28.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 44999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 129      |\n",
      "|    time_elapsed    | 548      |\n",
      "|    total_timesteps | 70815    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 101      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 45814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=-112.41 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -112     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -381     |\n",
      "|    critic_loss     | 43.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 45999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-123.00 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -123     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 60       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 46999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=-96.51 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -96.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 25.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 47999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=-68.64 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -68.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 74000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 40.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 48999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 126      |\n",
      "|    time_elapsed    | 591      |\n",
      "|    total_timesteps | 74815    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 23.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 49814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-41.20 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -41.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 34.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 49999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-57.32 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -57.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 76000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 39.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 50999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=-52.85 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -52.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 30.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 51999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=-30.03 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -30      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 78000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 33.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 52999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 124      |\n",
      "|    time_elapsed    | 631      |\n",
      "|    total_timesteps | 78815    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 25.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 53814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=-65.38 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -65.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 26.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 53999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-45.58 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -45.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 410      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 54999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=-122.39 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 22.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 55999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=-95.41 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -95.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 82000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 87.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 56999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 122      |\n",
      "|    time_elapsed    | 673      |\n",
      "|    total_timesteps | 82815    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 31.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 57814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=-93.57 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -93.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 39.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 57999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-105.45 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 84000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 43.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 58999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-53.43 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -53.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 20.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 59999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=32.06 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 32.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 86000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 55.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 60999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 121      |\n",
      "|    time_elapsed    | 713      |\n",
      "|    total_timesteps | 86815    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 35.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 61814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=-70.23 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -70.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 87000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 85.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 61999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-89.18 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -89.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 88000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 28.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 62999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=-54.32 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -54.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 89000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 41.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 63999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-80.84 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -80.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 42.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 64999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 119      |\n",
      "|    time_elapsed    | 757      |\n",
      "|    total_timesteps | 90815    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 38.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 65814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=-38.57 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -38.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 91000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 60.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 65999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=-87.83 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -87.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 92000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 86.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 66999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=132.03 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 132      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 93000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 2.13e+03 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 67999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=131.21 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 131      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 94000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 46.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 68999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 802      |\n",
      "|    total_timesteps | 94815    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -294     |\n",
      "|    critic_loss     | 50.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 69814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=1091.00 +/- 0.00\n",
      "Episode length: 971.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 971      |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 95000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -298     |\n",
      "|    critic_loss     | 75       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 69999    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=96000, episode_reward=49.12 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 49.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 86.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 70999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=7.56 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 7.56     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 97000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 49.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 71999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=-10.03 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 98000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 39.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 72999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 116      |\n",
      "|    time_elapsed    | 845      |\n",
      "|    total_timesteps | 98815    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -294     |\n",
      "|    critic_loss     | 24.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 73814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=-14.34 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -14.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 99000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -291     |\n",
      "|    critic_loss     | 54       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 73999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=1130.79 +/- 0.00\n",
      "Episode length: 933.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 933      |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 30.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 74999    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=101000, episode_reward=-28.11 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -28.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 101000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -294     |\n",
      "|    critic_loss     | 24       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 75999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=-14.23 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -14.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 102000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 73.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 76999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 115      |\n",
      "|    time_elapsed    | 891      |\n",
      "|    total_timesteps | 102815   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 21       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 77814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=103000, episode_reward=-42.16 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -42.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 103000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 96       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 77999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-44.52 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -44.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 104000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -295     |\n",
      "|    critic_loss     | 58.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 78999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=1133.61 +/- 0.00\n",
      "Episode length: 849.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 849      |\n",
      "|    mean_reward     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 105000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 46.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 79999    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=106000, episode_reward=-12.03 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -12      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 106000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 42.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 80999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 108      |\n",
      "|    fps             | 114      |\n",
      "|    time_elapsed    | 931      |\n",
      "|    total_timesteps | 106815   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -283     |\n",
      "|    critic_loss     | 85.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 81814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=10.62 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 10.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 107000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 3.61e+03 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 81999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-91.97 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -92      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 108000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -247     |\n",
      "|    critic_loss     | 1.03e+03 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 82999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=57.27 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 57.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 109000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -283     |\n",
      "|    critic_loss     | 87.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 83999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=60.20 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 60.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -253     |\n",
      "|    critic_loss     | 38.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 84999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 112      |\n",
      "|    fps             | 114      |\n",
      "|    time_elapsed    | 969      |\n",
      "|    total_timesteps | 110815   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -272     |\n",
      "|    critic_loss     | 110      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 85814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=-79.55 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -79.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 111000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 52.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 85999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=60.66 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 60.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 112000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -273     |\n",
      "|    critic_loss     | 37.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 86999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=-135.36 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -135     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 113000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -293     |\n",
      "|    critic_loss     | 77.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 87999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=-97.74 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -97.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 114000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -261     |\n",
      "|    critic_loss     | 934      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 88999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 116      |\n",
      "|    fps             | 113      |\n",
      "|    time_elapsed    | 1012     |\n",
      "|    total_timesteps | 114815   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -240     |\n",
      "|    critic_loss     | 51.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 89814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-11.06 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -11.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 115000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -286     |\n",
      "|    critic_loss     | 27.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 89999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-48.96 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -49      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 116000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -245     |\n",
      "|    critic_loss     | 615      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 90999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=-45.12 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -45.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 117000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -237     |\n",
      "|    critic_loss     | 29.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 91999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=-22.58 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -22.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 118000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -256     |\n",
      "|    critic_loss     | 214      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 92999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 112      |\n",
      "|    time_elapsed    | 1059     |\n",
      "|    total_timesteps | 118815   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -221     |\n",
      "|    critic_loss     | 20.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 93814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=-36.74 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -36.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 119000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -250     |\n",
      "|    critic_loss     | 30.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 93999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-95.91 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -95.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -239     |\n",
      "|    critic_loss     | 41.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 94999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=87.07 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 87.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 121000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -245     |\n",
      "|    critic_loss     | 24       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 95999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=53.30 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 53.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 122000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -230     |\n",
      "|    critic_loss     | 34.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 96999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 124      |\n",
      "|    fps             | 111      |\n",
      "|    time_elapsed    | 1104     |\n",
      "|    total_timesteps | 122815   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -206     |\n",
      "|    critic_loss     | 27.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 97814    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=19.74 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 19.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 123000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -237     |\n",
      "|    critic_loss     | 40       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 97999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=109.53 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 124000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -245     |\n",
      "|    critic_loss     | 27.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 98999    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=122.90 +/- 0.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 123      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 125000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -209     |\n",
      "|    critic_loss     | 53.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 99999    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_td3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 94.12176662794081, Std reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
